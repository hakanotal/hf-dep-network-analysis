# Project Overview: A Risk Evaluation Framework for Open-Source LLMs

## 1. Project Goal & Vision

This project aims to develop a novel risk evaluation framework for open-source Large Language Models (LLMs). By constructing and analyzing the HuggingFace ecosystem as a dynamic dependency graph network, the central objective is to quantify, model, and predict the potential risks associated with these models. The framework will analyze how risk propagates through the network based on model connections, community engagement, and intrinsic model properties.

## 2. Core Methodology: Graph-Based Risk Analysis

The project's foundation is a graph network where models are nodes and their relationships (e.g., finetuned, merged) are edges.

- **Project Status:** Significant progress has been made in data collection and network construction. An initial graph has been successfully created containing approximately 50,000 models, filtered for the “text-generation” pipeline, “English” language, and a minimum of 10 downloads.
- **Node Criticality & Edge Influence:** The analysis distinguishes between a model's importance and its influence:
    - **Importance (Centrality):** Measures how critical a model is to the ecosystem (e.g., a foundational model many others depend on). This is calculated by analyzing information flow *towards* the node.
    - **Influence (Impact):** Measures how a model's properties, including its risks, affect downstream models. This is calculated by analyzing information flow *from* the node. The impact score will be derived from metrics like download and like counts.

## 3. Multi-Faceted Risk Calculation

A model's final risk score will be a composite measure derived from multiple quantitative and qualitative sources. The ultimate deliverable is a risk propagation formula that models how these calculated scores move through the graph.

#### Data Sources for Risk Score:

- **Community-Reported Data:** Analyzing the number of issues, discussions, and comments on a model's Hugging Face page.
- **External Incidents:** Incorporating known security vulnerabilities or public reports of model misuse.
- **Empirical Testing:** Systematically evaluating models against a suite of benchmark datasets designed to test for safety, bias, security, and other critical risk categories. It gives an idea of how benchmark risk scores propagate between different nodes on the graph.
- **Vendor Risk-Score:** A vendor-specific risk score will be generated by leveraging web scraping to conduct real-time news analysis. This process will involve monitoring news articles, press releases, and public disclosures related to the vendors or labs that publish models. Natural Language Processing (NLP) techniques will be applied to this scraped data to perform sentiment analysis and identify any reported issues, such as security breaches, ethical controversies, or financial instability, contributing to a comprehensive vendor risk profile.

## 4. Data Enrichment & Analysis Dimensions

To create a rich, multi-layered analysis, the project will incorporate a wide array of metadata, including:

#### Model Lineage & Technical Specs:

- **Relationships:** Mapping connections between base models, fine-tuned models, quantized models, merged models and adapter models.
- **Architecture & Parameters:** Incorporating data on model architecture (e.g., Llama, Qwen, Gemma) and parameter count to analyze the evolution of model size and design.
- **Model Purpose/Specialty:** The framework will identify the intended purpose for which a model was trained or fine-tuned. By analyzing model cards and associated documentation, the specialty of a model (e.g., medical text summarization, code generation, conversational AI) will be categorized. This allows for a more granular risk assessment, as risks can be context-dependent and vary based on the model's application domain.

#### Community & Ecosystem Dynamics:

- **Engagement & Reputation:** Using metrics like “downloads” and “likes” to develop a "reputation score".
- **Vendor Analysis:** Exploring the ecosystems around specific vendors, labs, or contributors to map their release patterns.
- **Temporal Dynamics:** Leveraging “upload times” to create temporal graphs, studying how the LLM landscape evolves and the lifespan of models.

#### Temporal Dynamics & Timeline Insights:

- **Evolutionary Trends:** Leveraging “upload times” to create temporal graphs, studying how the LLM landscape evolves and the lifespan of models.
- **Impact Over Time:** The framework will analyze which models created the most significant impact upon their publication. This will be measured by observing sudden shifts in the dependency graph, such as a rapid increase in fine-tuning activities, a surge in community engagement metrics (downloads, likes, discussions), and citations in subsequent model cards. By tracking these temporal footprints, the project can identify historically influential models (e.g., the various releases of GPT, Llama, and BERT) and analyze the characteristics that contribute to their broad adoption and impact on the ecosystem.

#### Contextual Factors:

- **Origin & Language:** Analyzing the model's country of origin, developing institution, and primary language.
- **License Analysis:** Tracking the propagation of license types (e.g., MIT, OpenRAIL) and their correlation with model adoption.
- **Benchmark Results:** Calculate scores for various benchmarks for each model.

## 5. Key Challenges and Research Questions

The project has identified two primary challenges that are central to its research scope:

1.  **Missing Data:** Many models have incomplete metadata, such as a missing architecture field. The project will investigate other options or heuristics or the use of Bayesian statistical methods to intelligently infer and handle these missing values.
2.  **Popularity Bias:** A core research question has been identified: "Is the discovery of more issues in popular models a genuine reflection of higher risk, or is it a form of popularity bias, where more users naturally lead to more discovered flaws?" The framework must be designed to account for and isolate this potential bias in its risk calculations.

---

# Project Documentation: HF Models Dependency Graph Network Construction

The technical foundation of this project is a set of Python scripts that systematically collect, analyze, and map the Hugging Face model ecosystem. This process transforms raw model metadata into a structured, analyzable dependency graph.

### Data Collection & Initial Analysis:

The process begins by using the `huggingface_hub` API to fetch metadata for up to 50,000 models, filtered by criteria such as the `text-generation` task. For each model, comprehensive details like downloads, likes, tags, and configuration files are extracted.

### Relationship & Type Detection:

A critical function (`detect_model_type_and_base_models`) parses the metadata tags of each model. It uses regular expressions to identify specific relationship patterns (e.g., `base_model:finetune:meta-llama/Llama-2-7b-chat-hf`). This allows for the programmatic classification of each model as a base, finetuned, quantized, merged, or adapter model, and crucially, identifies its parent models.

### Network Graph Construction:

Using the `networkx` library, a directed graph (`DiGraph`) is constructed:

- **Nodes:** Each unique model ID becomes a node in the graph. Nodes are enriched with attributes like `model_type` and `model_arch`.
- **Edges:** A directed edge is created from a base model to its derivative. For example, an edge would point from `meta-llama/Llama-2-7b` to `TheBloke/Llama-2-7B-quantized`. Each edge is labeled with the relationship type (finetuned, quantized, etc.).

### Network Export:

The structured network data (nodes and edges with their attributes) is exported into `edges.csv` and `metadata.csv` files, making it compatible with external graph analysis and visualization tools like Gephi.

A dynamic visualization of the network is generated using `matplotlib`. In this visualization, nodes are colored by their core architecture to reveal technological communities, and their shape indicates their model type (base, finetuned, etc.), providing an immediate, high-level view of the LLM ecosystem's structure and dependencies.

### [Cosmograph Visualization Link](https://cosmograph.app/run/?data=https://raw.githubusercontent.com/hakanotal/hf-dep-network-analysis/refs/heads/main/network_data/edges.csv&meta=https://raw.githubusercontent.com/hakanotal/hf-dep-network-analysis/refs/heads/main/network_data/metadata.csv&source=source&target=target&gravity=0.4&repulsion=1&repulsionTheta=0.5&linkSpring=0.06&linkDistance=10&friction=0.5&renderLabels=true&renderHoveredLabel=true&renderLinks=true&linkArrows=true&nodeSizeScale=1&linkWidthScale=1&linkArrowsSizeScale=1&nodeSize=size-downloads&nodeColor=color-model_arch&linkWidth=width-default&linkColor=color-default)
